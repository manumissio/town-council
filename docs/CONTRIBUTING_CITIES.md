# Adding a New City Crawler

## Overview
Add city coverage by implementing a spider and ensuring metadata seeding/indexing paths include the city.

Note: GitHub Pages demo data is static fixture content and is not generated by spiders. City ingestion changes must still be validated in the Docker pipeline flow below.

## Steps

### 1) Add city metadata
Update `city_metadata/list_of_cities.csv` with:
- `ocd_division_id`
- display name
- crawler metadata
- hosting service fields (for example `granicus|legistar|city`)

### 2) Implement spider
Create spider file under:
- `council_crawler/council_crawler/spiders/`

Prefer existing templates/base classes when possible:
- `BaseCitySpider`
- Legistar templates in `council_crawler/templates/`

### 3) Validate crawler output contract
Ensure emitted items include:
- meeting/event identity
- valid record date
- source URL
- document links (agenda/minutes where available)

Important validation behavior:
- `record_date` must be a real Python `date` value in crawler output.
- Invalid `record_date` values are dropped by crawler validation in
  `council_crawler/council_crawler/pipelines.py`.

### 4) Seed + process
```bash
docker compose run --rm pipeline python seed_places.py
docker compose run --rm crawler scrapy crawl <city_spider_name>
docker compose run --rm pipeline python run_pipeline.py
```

### 5) Verify in API/UI
- `/metadata` includes city facet
- city meetings appear in search
- opening a record shows extracted content

## Test expectations
Add/extend tests with invariant checks (not brittle exact output):
- seed places includes city record
- spider handles missing optional doc links cleanly
- mapping from source payload -> internal fields remains valid

## Related Docs
- Crawler internals: `council_crawler/council_crawler_readme.md`
- Operator runbook: `docs/OPERATIONS.md`
