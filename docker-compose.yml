services:
  # PostgreSQL Database: Replaces SQLite for production-grade concurrency and performance.
  postgres:
    image: pgvector/pgvector:pg15
    command: postgres -c 'max_connections=200'
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-town_council}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-secure_dev_password}
      POSTGRES_DB: ${POSTGRES_DB:-town_council_db}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Service for running the Scrapy spiders
  crawler:
    build: .
    volumes:
      - .:/app
      - data:/app/data
    working_dir: /app/council_crawler
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - postgres

  # Service for running the downloader pipeline
  pipeline:
    build: .
    volumes:
      - .:/app
      - data:/app/data
    working_dir: /app/pipeline
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
      - APP_ENV=${APP_ENV:-dev}
      - STARTUP_PURGE_DERIVED=${STARTUP_PURGE_DERIVED:-true}
      - LOCAL_AI_BACKEND=${LOCAL_AI_BACKEND:-http}
      - LOCAL_AI_HTTP_BASE_URL=${LOCAL_AI_HTTP_BASE_URL:-http://inference:11434}
      - LOCAL_AI_HTTP_TIMEOUT_SECONDS=${LOCAL_AI_HTTP_TIMEOUT_SECONDS:-60}
      - LOCAL_AI_HTTP_MAX_RETRIES=${LOCAL_AI_HTTP_MAX_RETRIES:-1}
      - LOCAL_AI_HTTP_MODEL=${LOCAL_AI_HTTP_MODEL:-gemma-3-270m-custom}
      # Semantic search (Milestone B) config. Endpoint is disabled by default.
      - SEMANTIC_ENABLED=${SEMANTIC_ENABLED:-false}
      - SEMANTIC_BACKEND=${SEMANTIC_BACKEND:-faiss}
      - SEMANTIC_MODEL_NAME=${SEMANTIC_MODEL_NAME:-all-MiniLM-L6-v2}
      - SEMANTIC_INDEX_DIR=${SEMANTIC_INDEX_DIR:-/app/data/semantic}
      - SEMANTIC_CONTENT_MAX_CHARS=${SEMANTIC_CONTENT_MAX_CHARS:-4000}
      - SEMANTIC_BASE_TOP_K=${SEMANTIC_BASE_TOP_K:-200}
      - SEMANTIC_MAX_TOP_K=${SEMANTIC_MAX_TOP_K:-5000}
      - SEMANTIC_FILTER_EXPANSION_FACTOR=${SEMANTIC_FILTER_EXPANSION_FACTOR:-4}
      - SEMANTIC_REQUIRE_SINGLE_PROCESS=${SEMANTIC_REQUIRE_SINGLE_PROCESS:-true}
      - SEMANTIC_ALLOW_MULTIPROCESS=${SEMANTIC_ALLOW_MULTIPROCESS:-false}
      - SEMANTIC_REQUIRE_FAISS=${SEMANTIC_REQUIRE_FAISS:-false}
      # Enable OCR *only as a fallback* when a PDF has little/no selectable text.
      # This makes scanned PDFs usable without forcing OCR on every document.
      - TIKA_OCR_FALLBACK_ENABLED=true
    depends_on:
      - postgres
      # The pipeline's final step indexes into Meilisearch. `docker compose run pipeline ...`
      # should bring up the search service automatically for a smooth first-run experience.
      - meilisearch
      - redis
      - tika
      - inference

  # Apache Tika Server for high-performance text extraction.
  tika:
    image: apache/tika:latest-full
    restart: always
    mem_limit: 4G
    environment:
      - TIKA_CHILD_JVM_OPTS=-Xmx3g
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9998/tika"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Service for extracting text from downloaded documents.
  extractor:
    build: .
    volumes:
      - .:/app
      - data:/app/data
    working_dir: /app/pipeline
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - TIKA_SERVER_ENDPOINT=http://tika:9998
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
      - TIKA_OCR_FALLBACK_ENABLED=true
    depends_on:
      - tika
      - postgres

  # Celery Worker: Handles compute-intensive AI tasks (Summarization, Segmentation)
  # in the background so the API remains fast and responsive.
  worker:
    build: .
    volumes:
      - .:/app
      - data:/app/data
    working_dir: /app
    # Conservative D2-lite default: use HTTP inference backend with bounded parallelism.
    # Why this branch exists: lifting concurrency while keeping in-process llama.cpp would
    # duplicate model memory per process and destabilize shared hosts.
    command: celery -A pipeline.tasks worker --loglevel=info --concurrency=${WORKER_CONCURRENCY:-3} --pool=${WORKER_POOL:-prefork}
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - MEILI_HOST=${MEILI_HOST:-http://meilisearch:7700}
      - MEILI_MASTER_KEY=${MEILI_MASTER_KEY:-masterKey}
      - REDIS_HOST=redis
      - REDIS_PASSWORD=${REDIS_PASSWORD:-secure_redis_password}
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD:-secure_redis_password}@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD:-secure_redis_password}@redis:6379/0
      - DATA_DIR=/app/data
      - TC_WORKER_METRICS_PORT=8001
      - APP_ENV=${APP_ENV:-dev}
      - STARTUP_PURGE_DERIVED=${STARTUP_PURGE_DERIVED:-true}
      - LOCAL_AI_BACKEND=${LOCAL_AI_BACKEND:-http}
      - LOCAL_AI_HTTP_BASE_URL=${LOCAL_AI_HTTP_BASE_URL:-http://inference:11434}
      - LOCAL_AI_HTTP_TIMEOUT_SECONDS=${LOCAL_AI_HTTP_TIMEOUT_SECONDS:-60}
      - LOCAL_AI_HTTP_MAX_RETRIES=${LOCAL_AI_HTTP_MAX_RETRIES:-1}
      - LOCAL_AI_HTTP_MODEL=${LOCAL_AI_HTTP_MODEL:-gemma-3-270m-custom}
      # Local AI (Gemma 3) tuning. Defaults are safe but configurable.
      - LLM_CONTEXT_WINDOW=${LLM_CONTEXT_WINDOW:-16384}
      - LLM_SUMMARY_MAX_TEXT=${LLM_SUMMARY_MAX_TEXT:-30000}
      - LLM_SUMMARY_MAX_TOKENS=${LLM_SUMMARY_MAX_TOKENS:-512}
      - LLM_AGENDA_MAX_TEXT=${LLM_AGENDA_MAX_TEXT:-60000}
      # Semantic search guardrails/config mirrored here for worker-side diagnostics/tasks.
      - SEMANTIC_ENABLED=${SEMANTIC_ENABLED:-false}
      - SEMANTIC_BACKEND=${SEMANTIC_BACKEND:-faiss}
      - SEMANTIC_MODEL_NAME=${SEMANTIC_MODEL_NAME:-all-MiniLM-L6-v2}
      - SEMANTIC_INDEX_DIR=${SEMANTIC_INDEX_DIR:-/app/data/semantic}
      - SEMANTIC_CONTENT_MAX_CHARS=${SEMANTIC_CONTENT_MAX_CHARS:-4000}
      - SEMANTIC_BASE_TOP_K=${SEMANTIC_BASE_TOP_K:-200}
      - SEMANTIC_MAX_TOP_K=${SEMANTIC_MAX_TOP_K:-5000}
      - SEMANTIC_FILTER_EXPANSION_FACTOR=${SEMANTIC_FILTER_EXPANSION_FACTOR:-4}
      - SEMANTIC_REQUIRE_SINGLE_PROCESS=${SEMANTIC_REQUIRE_SINGLE_PROCESS:-true}
      - SEMANTIC_ALLOW_MULTIPROCESS=${SEMANTIC_ALLOW_MULTIPROCESS:-false}
      - SEMANTIC_REQUIRE_FAISS=${SEMANTIC_REQUIRE_FAISS:-false}
    depends_on:
      - redis
      - postgres
      - tika
      - inference

  # NLP Worker: Uses SpaCy to extract Organizations and Locations from text.
  nlp:
    build: .
    volumes:
      - .:/app
      - data:/app/data
    working_dir: /app/pipeline
    command: python nlp_worker.py
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - postgres

  # Table Worker: Uses Camelot to extract structured tables from PDFs.
  tables:
    build: .
    volumes:
      - .:/app
      - data:/app/data
    working_dir: /app/pipeline
    command: python table_worker.py
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - postgres

  # Topic Modeler: Uses LDA to discover themes across all meeting minutes.
  topics:
    build: .
    volumes:
      - .:/app
      - data:/app/data
    working_dir: /app/pipeline
    command: python topic_worker.py
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - postgres

  # Meilisearch: A fast, typo-tolerant search engine (FOSS).
  meilisearch:
    image: getmeili/meilisearch:v1.6
    ports:
      - "7700:7700"
    environment:
      - MEILI_MASTER_KEY=${MEILI_MASTER_KEY:-masterKey}
    volumes:
      - meili_data:/meili_data

  # PERFORMANCE: Redis Caching Layer
  # Stores frequently accessed data (like city lists) in RAM for instant access.
  # This reduces database load by ~95% for read-heavy operations.
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --requirepass ${REDIS_PASSWORD:-secure_redis_password} --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # FastAPI Backend: Serves the search API and data to users.
  api:
    build: .
    volumes:
      - .:/app
      - data:/app/data
    working_dir: /app/api
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    environment:
      - MEILI_HOST=${MEILI_HOST:-http://meilisearch:7700}
      - MEILI_MASTER_KEY=${MEILI_MASTER_KEY:-masterKey}
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - REDIS_HOST=redis
      - REDIS_PASSWORD=${REDIS_PASSWORD:-secure_redis_password}
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD:-secure_redis_password}@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD:-secure_redis_password}@redis:6379/0
      - PYTHONPATH=/app
      - API_AUTH_KEY=${API_AUTH_KEY:-dev_secret_key_change_me}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-http://localhost:3000}
      - APP_ENV=${APP_ENV:-dev}
      - STARTUP_PURGE_DERIVED=${STARTUP_PURGE_DERIVED:-true}
      - FEATURE_TRENDS_DASHBOARD=${FEATURE_TRENDS_DASHBOARD:-false}
      - LOCAL_AI_BACKEND=${LOCAL_AI_BACKEND:-http}
      - LOCAL_AI_HTTP_BASE_URL=${LOCAL_AI_HTTP_BASE_URL:-http://inference:11434}
      - LOCAL_AI_HTTP_TIMEOUT_SECONDS=${LOCAL_AI_HTTP_TIMEOUT_SECONDS:-60}
      - LOCAL_AI_HTTP_MAX_RETRIES=${LOCAL_AI_HTTP_MAX_RETRIES:-1}
      - LOCAL_AI_HTTP_MODEL=${LOCAL_AI_HTTP_MODEL:-gemma-3-270m-custom}
      # Semantic search API settings.
      - SEMANTIC_ENABLED=${SEMANTIC_ENABLED:-false}
      - SEMANTIC_BACKEND=${SEMANTIC_BACKEND:-faiss}
      - SEMANTIC_MODEL_NAME=${SEMANTIC_MODEL_NAME:-all-MiniLM-L6-v2}
      - SEMANTIC_INDEX_DIR=${SEMANTIC_INDEX_DIR:-/app/data/semantic}
      - SEMANTIC_CONTENT_MAX_CHARS=${SEMANTIC_CONTENT_MAX_CHARS:-4000}
      - SEMANTIC_BASE_TOP_K=${SEMANTIC_BASE_TOP_K:-200}
      - SEMANTIC_MAX_TOP_K=${SEMANTIC_MAX_TOP_K:-5000}
      - SEMANTIC_FILTER_EXPANSION_FACTOR=${SEMANTIC_FILTER_EXPANSION_FACTOR:-4}
      - SEMANTIC_REQUIRE_SINGLE_PROCESS=${SEMANTIC_REQUIRE_SINGLE_PROCESS:-true}
      - SEMANTIC_ALLOW_MULTIPROCESS=${SEMANTIC_ALLOW_MULTIPROCESS:-false}
      - SEMANTIC_REQUIRE_FAISS=${SEMANTIC_REQUIRE_FAISS:-false}
    depends_on:
      - meilisearch
      - postgres
      - redis
      - inference

  # Dedicated inference runtime for D2-lite (HTTP backend).
  inference:
    image: ollama/ollama:latest
    command: ["serve"]
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      # Infra-level concurrency control: keep app layer hardware-agnostic.
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-1}
    mem_limit: 4G
    cpus: 2.0
    volumes:
      - ollama_data:/root/.ollama

  # Next.js Frontend: The user interface for searching and viewing results.
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
        - NEXT_PUBLIC_API_AUTH_KEY=${NEXT_PUBLIC_API_AUTH_KEY:-}
        - NEXT_PUBLIC_FEATURE_TRENDS_DASHBOARD=${NEXT_PUBLIC_FEATURE_TRENDS_DASHBOARD:-false}
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      - NEXT_PUBLIC_API_AUTH_KEY=${NEXT_PUBLIC_API_AUTH_KEY:-}
      - NEXT_PUBLIC_FEATURE_TRENDS_DASHBOARD=${NEXT_PUBLIC_FEATURE_TRENDS_DASHBOARD:-false}
    depends_on:
      - api

  # Prometheus: FOSS monitoring system and time series database.
  prometheus:
    image: prom/prometheus:latest
    # Allow config reloads without container restarts (POST /-/reload).
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --web.enable-lifecycle
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"

  # Grafana: FOSS analytics and interactive visualization web application.
  grafana:
    image: grafana/grafana:latest
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "3001:3000" # Mapped to 3001 to avoid conflict with frontend
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus

  # Prometheus exporter for Postgres (connections, locks, query stats).
  postgres_exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    environment:
      - DATA_SOURCE_NAME=postgresql://${POSTGRES_USER:-town_council}:${POSTGRES_PASSWORD:-secure_dev_password}@postgres:5432/${POSTGRES_DB:-town_council_db}?sslmode=disable
    depends_on:
      - postgres

  # Prometheus exporter for Redis (memory, ops/sec, keyspace stats).
  redis_exporter:
    image: oliver006/redis_exporter:v1.63.0
    environment:
      - REDIS_ADDR=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-secure_redis_password}
    depends_on:
      - redis

  # Container metrics (CPU/memory/network). Useful for spotting resource bottlenecks quickly.
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    privileged: true
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    restart: unless-stopped

  # Monitor: Custom Python service to export application metrics to Prometheus.
  monitor:
    build: .
    volumes:
      - .:/app
      - data:/app/data
    working_dir: /app/pipeline
    command: python monitor.py
    environment:
      - PYTHONUNBUFFERED=1
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - PYTHONPATH=/app
    depends_on:
      - postgres

volumes:
  # Persistent data storage for the SQLite DB and downloaded PDFs
  data:
  # Persistent storage for the search index
  meili_data:
  # Persistent storage for the Postgres database
  postgres_data:
  # Persistent storage for monitoring data
  prometheus_data:
  grafana_data:
  # Persistent storage for Redis cache
  redis_data:
  # Persistent storage for inference model blobs
  ollama_data:
