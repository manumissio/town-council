services:
  # PostgreSQL Database: Replaces SQLite for production-grade concurrency and performance.
  postgres:
    image: postgres:15
    command: postgres -c 'max_connections=200'
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-town_council}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-secure_dev_password}
      POSTGRES_DB: ${POSTGRES_DB:-town_council_db}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Service for running the Scrapy spiders
  crawler:
    build: .
    volumes:
      - .:/app
    working_dir: /app/council_crawler
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - postgres

  # Service for running the downloader pipeline
  pipeline:
    build: .
    volumes:
      - .:/app
    working_dir: /app/pipeline
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - postgres

  # Apache Tika Server for high-performance text extraction and OCR.
  tika:
    image: apache/tika:latest-full
    restart: always
    mem_limit: 4G
    environment:
      - TIKA_CHILD_JVM_OPTS=-Xmx3g
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9998/tika"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Service for extracting text from downloaded documents.
  extractor:
    build: .
    volumes:
      - .:/app
    working_dir: /app/pipeline
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - TIKA_SERVER_ENDPOINT=http://tika:9998
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - tika
      - postgres

  # Celery Worker: Handles compute-intensive AI tasks (Summarization, Segmentation)
  # in the background so the API remains fast and responsive.
  worker:
    build: .
    volumes:
      - .:/app
    working_dir: /app
    command: celery -A pipeline.tasks worker --loglevel=info --concurrency=1
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - REDIS_HOST=redis
      - REDIS_PASSWORD=${REDIS_PASSWORD:-secure_redis_password}
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD:-secure_redis_password}@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD:-secure_redis_password}@redis:6379/0
      - DATA_DIR=/app/data
    depends_on:
      - redis
      - postgres
      - tika

  # NLP Worker: Uses SpaCy to extract Organizations and Locations from text.
  nlp:
    build: .
    volumes:
      - .:/app
    working_dir: /app/pipeline
    command: python nlp_worker.py
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - postgres

  # Table Worker: Uses Camelot to extract structured tables from PDFs.
  tables:
    build: .
    volumes:
      - .:/app
    working_dir: /app/pipeline
    command: python table_worker.py
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - postgres

  # Topic Modeler: Uses LDA to discover themes across all meeting minutes.
  topics:
    build: .
    volumes:
      - .:/app
    working_dir: /app/pipeline
    command: python topic_worker.py
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - DATA_DIR=/app/data
    depends_on:
      - postgres

  # Meilisearch: A fast, typo-tolerant search engine (FOSS).
  meilisearch:
    image: getmeili/meilisearch:v1.6
    ports:
      - "7700:7700"
    environment:
      - MEILI_MASTER_KEY=${MEILI_MASTER_KEY:-masterKey}
    volumes:
      - meili_data:/meili_data

  # PERFORMANCE: Redis Caching Layer
  # Stores frequently accessed data (like city lists) in RAM for instant access.
  # This reduces database load by ~95% for read-heavy operations.
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --requirepass ${REDIS_PASSWORD:-secure_redis_password} --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # FastAPI Backend: Serves the search API and data to users.
  api:
    build: .
    volumes:
      - .:/app
    working_dir: /app/api
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    environment:
      - MEILI_HOST=${MEILI_HOST:-http://meilisearch:7700}
      - MEILI_MASTER_KEY=${MEILI_MASTER_KEY:-masterKey}
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - REDIS_HOST=redis
      - REDIS_PASSWORD=${REDIS_PASSWORD:-secure_redis_password}
      - CELERY_BROKER_URL=redis://:${REDIS_PASSWORD:-secure_redis_password}@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:${REDIS_PASSWORD:-secure_redis_password}@redis:6379/0
      - PYTHONPATH=/app
      - API_AUTH_KEY=${API_AUTH_KEY:-dev_secret_key_change_me}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-http://localhost:3000}
    depends_on:
      - meilisearch
      - postgres
      - redis

  # Next.js Frontend: The user interface for searching and viewing results.
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      - API_AUTH_KEY=${API_AUTH_KEY:-dev_secret_key_change_me}
    depends_on:
      - api

  # Prometheus: FOSS monitoring system and time series database.
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"

  # Grafana: FOSS analytics and interactive visualization web application.
  grafana:
    image: grafana/grafana:latest
    volumes:
      - grafana_data:/var/lib/grafana
    ports:
      - "3001:3000" # Mapped to 3001 to avoid conflict with frontend
    depends_on:
      - prometheus

  # Monitor: Custom Python service to export application metrics to Prometheus.
  monitor:
    build: .
    volumes:
      - .:/app
    working_dir: /app/pipeline
    command: python monitor.py
    environment:
      - PYTHONUNBUFFERED=1
      - DATABASE_URL=${DATABASE_URL:-postgresql://town_council:secure_dev_password@postgres:5432/town_council_db}
      - PYTHONPATH=/app
    depends_on:
      - postgres

volumes:
  # Persistent data storage for the SQLite DB and downloaded PDFs
  data:
  # Persistent storage for the search index
  meili_data:
  # Persistent storage for the Postgres database
  postgres_data:
  # Persistent storage for monitoring data
  prometheus_data:
  grafana_data:
  # Persistent storage for Redis cache
  redis_data:
